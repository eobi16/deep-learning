{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7PPg0VZG5cK",
        "outputId": "35b303ae-26ad-446e-e7ef-bac00b51f7f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libgl1-mesa-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "libgl1-mesa-dev set to manually installed.\n",
            "software-properties-common is already the newest version (0.96.24.32.18).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  glew-utils\n",
            "The following NEW packages will be installed:\n",
            "  libgl1-mesa-glx libglew-dev libglew2.0 libosmesa6 libosmesa6-dev\n",
            "0 upgraded, 5 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 2,916 kB of archives.\n",
            "After this operation, 12.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-glx amd64 20.0.8-0ubuntu1~18.04.1 [5,532 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglew2.0 amd64 2.0.0-5 [140 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglew-dev amd64 2.0.0-5 [120 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libosmesa6 amd64 20.0.8-0ubuntu1~18.04.1 [2,641 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libosmesa6-dev amd64 20.0.8-0ubuntu1~18.04.1 [8,828 B]\n",
            "Fetched 2,916 kB in 0s (6,648 kB/s)\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 155632 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libglew2.0:amd64.\n",
            "Preparing to unpack .../libglew2.0_2.0.0-5_amd64.deb ...\n",
            "Unpacking libglew2.0:amd64 (2.0.0-5) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../libglew-dev_2.0.0-5_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.0.0-5) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../libosmesa6_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../libosmesa6-dev_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libosmesa6:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libglew2.0:amd64 (2.0.0-5) ...\n",
            "Setting up libglew-dev:amd64 (2.0.0-5) ...\n",
            "Setting up libosmesa6-dev:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  patchelf\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 46.5 kB of archives.\n",
            "After this operation, 130 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 patchelf amd64 0.9-1 [46.5 kB]\n",
            "Fetched 46.5 kB in 0s (285 kB/s)\n",
            "Selecting previously unselected package patchelf.\n",
            "(Reading database ... 155670 files and directories currently installed.)\n",
            "Preparing to unpack .../patchelf_0.9-1_amd64.deb ...\n",
            "Unpacking patchelf (0.9-1) ...\n",
            "Setting up patchelf (0.9-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w7tslZCHl0T",
        "outputId": "a3be07cb-321c-429b-ddba-1d847e5686d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qeAEFtTG8dV",
        "outputId": "995ac503-6efa-4975-d1f8-327d8e80d263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting free-mujoco-py\n",
            "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting imageio<3.0.0,>=2.9.0\n",
            "  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (0.29.30)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.15.0)\n",
            "Collecting fasteners==0.15\n",
            "  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
            "Collecting glfw<2.0.0,>=1.4.0\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[K     |████████████████████████████████| 203 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.21.6)\n",
            "Collecting monotonic>=0.1\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fasteners==0.15->free-mujoco-py) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n",
            "Collecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 48.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow, monotonic, imageio, glfw, fasteners, free-mujoco-py\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 imageio-2.19.3 monotonic-1.6 pillow-9.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install free-mujoco-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHtnurnyIwaZ"
      },
      "source": [
        "**Now you need to restart the runtime as numpy is apparently automatically imported...**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtWrmOCKHD6I",
        "outputId": "3a777c01-1008-4e3b-aa9e-81dd995afde6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.pyx because it changed.\n",
            "[1/1] Cythonizing /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.pyx\n",
            "running build_ext\n",
            "building 'mujoco_py.cymj' extension\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/mujoco_py -I/usr/local/lib/python3.7/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.o -fopenmp -w\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/mujoco_py -I/usr/local/lib/python3.7/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c /usr/local/lib/python3.7/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl/osmesashim.o -L/usr/local/lib/python3.7/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -Wl,--enable-new-dtags,-R/usr/local/lib/python3.7/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py/cymj.cpython-37m-x86_64-linux-gnu.so -fopenmp\n"
          ]
        }
      ],
      "source": [
        "import mujoco_py\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zibrUFrHI50",
        "outputId": "0824cdd0-2a42-4e26-a083-d7044fe3b84e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8,)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Swimmer-v3\")\n",
        "\n",
        "print(env.reset().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udtkZhXtHPzT",
        "outputId": "d31043c5-1547-49be-842c-ddcc327bbe5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting sb3-contrib\n",
            "  Downloading sb3_contrib-1.5.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.11.0+cu113)\n",
            "Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3[extra]) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3[extra]) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (9.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Collecting ale-py~=0.7.4\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 51.0 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.46.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616827 sha256=ccd8d943b6ebec4e3a6b3dbd08c6de398cf254790cb1208515707b1fe47de03d\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=2a74facb1889eb4760798b0b21975af5ce2bb6e81bfad1b57fa83399b72ff65e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym, AutoROM.accept-rom-license, autorom, stable-baselines3, ale-py, sb3-contrib\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0 sb3-contrib-1.5.0 stable-baselines3-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra] sb3-contrib pip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4rgFSKgHO7a",
        "outputId": "0b0c9e1c-ab7b-4d79-eb0d-fcc0646ddca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'Swimmer-v3'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 8.57     |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 166      |\n",
            "|    time_elapsed    | 23       |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.789   |\n",
            "|    critic_loss     | 0.0122   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 14.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 145      |\n",
            "|    time_elapsed    | 54       |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.16    |\n",
            "|    critic_loss     | 0.0258   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 14.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 140      |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.27    |\n",
            "|    critic_loss     | 0.0205   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.32     |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 137      |\n",
            "|    time_elapsed    | 116      |\n",
            "|    total_timesteps | 16000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -10.5    |\n",
            "|    critic_loss     | 0.058    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.47     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 136      |\n",
            "|    time_elapsed    | 146      |\n",
            "|    total_timesteps | 20000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -9.65    |\n",
            "|    critic_loss     | 0.0273   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -1.87    |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 177      |\n",
            "|    total_timesteps | 24000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.42    |\n",
            "|    critic_loss     | 0.0284   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -5.03    |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 207      |\n",
            "|    total_timesteps | 28000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -6.62    |\n",
            "|    critic_loss     | 0.0298   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -6.47    |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 238      |\n",
            "|    total_timesteps | 32000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -4.7     |\n",
            "|    critic_loss     | 0.0264   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 31000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -8.04    |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 268      |\n",
            "|    total_timesteps | 36000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -3.63    |\n",
            "|    critic_loss     | 0.0242   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 35000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -9.12    |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 298      |\n",
            "|    total_timesteps | 40000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -2.56    |\n",
            "|    critic_loss     | 0.0226   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 39000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -9.74    |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 329      |\n",
            "|    total_timesteps | 44000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -2.06    |\n",
            "|    critic_loss     | 0.0192   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 43000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -10.7    |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 359      |\n",
            "|    total_timesteps | 48000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.41    |\n",
            "|    critic_loss     | 0.0155   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 47000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -11.6    |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 389      |\n",
            "|    total_timesteps | 52000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.08    |\n",
            "|    critic_loss     | 0.0141   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 51000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -12.1    |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 419      |\n",
            "|    total_timesteps | 56000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.859   |\n",
            "|    critic_loss     | 0.0127   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 55000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -12.5    |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 449      |\n",
            "|    total_timesteps | 60000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.535   |\n",
            "|    critic_loss     | 0.0146   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 59000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -13      |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 479      |\n",
            "|    total_timesteps | 64000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.07     |\n",
            "|    critic_loss     | 0.0143   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 63000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -13.3    |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 509      |\n",
            "|    total_timesteps | 68000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.675    |\n",
            "|    critic_loss     | 0.0133   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 67000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -13.8    |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 539      |\n",
            "|    total_timesteps | 72000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.811    |\n",
            "|    critic_loss     | 0.0163   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 71000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -14.1    |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 569      |\n",
            "|    total_timesteps | 76000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.15     |\n",
            "|    critic_loss     | 0.022    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 75000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -14.5    |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 599      |\n",
            "|    total_timesteps | 80000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.44     |\n",
            "|    critic_loss     | 0.0174   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 79000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -14.8    |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 629      |\n",
            "|    total_timesteps | 84000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.53     |\n",
            "|    critic_loss     | 0.0154   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 83000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -15      |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 659      |\n",
            "|    total_timesteps | 88000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.6      |\n",
            "|    critic_loss     | 0.0155   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 87000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -15      |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 688      |\n",
            "|    total_timesteps | 92000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.43     |\n",
            "|    critic_loss     | 0.0217   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 91000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -14.7    |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 718      |\n",
            "|    total_timesteps | 96000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.931    |\n",
            "|    critic_loss     | 0.0284   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 95000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -13.4    |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 748      |\n",
            "|    total_timesteps | 100000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.45    |\n",
            "|    critic_loss     | 0.0342   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 99000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -12.4    |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 104000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -5.56    |\n",
            "|    critic_loss     | 0.0244   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 103000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -12      |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 808      |\n",
            "|    total_timesteps | 108000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.19    |\n",
            "|    critic_loss     | 0.0256   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 107000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -11.5    |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 838      |\n",
            "|    total_timesteps | 112000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17.3    |\n",
            "|    critic_loss     | 0.254    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 111000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -10.1    |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 867      |\n",
            "|    total_timesteps | 116000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -38.1    |\n",
            "|    critic_loss     | 0.535    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 115000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -8.82    |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 897      |\n",
            "|    total_timesteps | 120000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.6    |\n",
            "|    critic_loss     | 0.163    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 119000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -7.51    |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 927      |\n",
            "|    total_timesteps | 124000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.2    |\n",
            "|    critic_loss     | 0.0768   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 123000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -6.56    |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 956      |\n",
            "|    total_timesteps | 128000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -36.8    |\n",
            "|    critic_loss     | 0.0868   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 127000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -6.46    |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 986      |\n",
            "|    total_timesteps | 132000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -36.7    |\n",
            "|    critic_loss     | 0.109    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 131000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -6.64    |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 1016     |\n",
            "|    total_timesteps | 136000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -33.9    |\n",
            "|    critic_loss     | 0.0772   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 135000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -6.27    |\n",
            "| time/              |          |\n",
            "|    episodes        | 140      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 1046     |\n",
            "|    total_timesteps | 140000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -33.7    |\n",
            "|    critic_loss     | 0.0513   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 139000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -6.16    |\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 1075     |\n",
            "|    total_timesteps | 144000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -31.8    |\n",
            "|    critic_loss     | 0.0457   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 143000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -5.55    |\n",
            "| time/              |          |\n",
            "|    episodes        | 148      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 1105     |\n",
            "|    total_timesteps | 148000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -29.2    |\n",
            "|    critic_loss     | 0.0329   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 147000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -5.14    |\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 1134     |\n",
            "|    total_timesteps | 152000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -26.6    |\n",
            "|    critic_loss     | 0.0322   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 151000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -4.29    |\n",
            "| time/              |          |\n",
            "|    episodes        | 156      |\n",
            "|    fps             | 133      |\n",
            "|    time_elapsed    | 1164     |\n",
            "|    total_timesteps | 156000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.9    |\n",
            "|    critic_loss     | 0.0354   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 155000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -2.94    |\n",
            "| time/              |          |\n",
            "|    episodes        | 160      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1193     |\n",
            "|    total_timesteps | 160000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -27.7    |\n",
            "|    critic_loss     | 0.0899   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 159000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -0.923   |\n",
            "| time/              |          |\n",
            "|    episodes        | 164      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1223     |\n",
            "|    total_timesteps | 164000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.7    |\n",
            "|    critic_loss     | 0.0466   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 163000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.08     |\n",
            "| time/              |          |\n",
            "|    episodes        | 168      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1253     |\n",
            "|    total_timesteps | 168000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.6    |\n",
            "|    critic_loss     | 0.0425   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 167000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.3      |\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1282     |\n",
            "|    total_timesteps | 172000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -21.4    |\n",
            "|    critic_loss     | 0.0385   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 171000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.52     |\n",
            "| time/              |          |\n",
            "|    episodes        | 176      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1312     |\n",
            "|    total_timesteps | 176000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.1    |\n",
            "|    critic_loss     | 0.0412   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 175000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.73     |\n",
            "| time/              |          |\n",
            "|    episodes        | 180      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1342     |\n",
            "|    total_timesteps | 180000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.2    |\n",
            "|    critic_loss     | 0.0367   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 179000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 10       |\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1371     |\n",
            "|    total_timesteps | 184000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19      |\n",
            "|    critic_loss     | 0.0431   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 183000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 12.1     |\n",
            "| time/              |          |\n",
            "|    episodes        | 188      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1401     |\n",
            "|    total_timesteps | 188000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.6    |\n",
            "|    critic_loss     | 0.0515   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 187000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 13.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1430     |\n",
            "|    total_timesteps | 192000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.5    |\n",
            "|    critic_loss     | 0.115    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 191000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 15.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 196      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1460     |\n",
            "|    total_timesteps | 196000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.2    |\n",
            "|    critic_loss     | 0.103    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 195000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 16.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 200      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1489     |\n",
            "|    total_timesteps | 200000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24      |\n",
            "|    critic_loss     | 0.0835   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 199000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 16.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 204      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1519     |\n",
            "|    total_timesteps | 204000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.9    |\n",
            "|    critic_loss     | 0.0647   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 203000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 16.8     |\n",
            "| time/              |          |\n",
            "|    episodes        | 208      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1548     |\n",
            "|    total_timesteps | 208000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24      |\n",
            "|    critic_loss     | 0.0643   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 207000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 17       |\n",
            "| time/              |          |\n",
            "|    episodes        | 212      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1577     |\n",
            "|    total_timesteps | 212000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24.2    |\n",
            "|    critic_loss     | 0.0579   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 211000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 17.8     |\n",
            "| time/              |          |\n",
            "|    episodes        | 216      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1607     |\n",
            "|    total_timesteps | 216000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24.5    |\n",
            "|    critic_loss     | 0.0655   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 215000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 18.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 220      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1636     |\n",
            "|    total_timesteps | 220000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24.8    |\n",
            "|    critic_loss     | 0.0628   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 219000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 19.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 224      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1666     |\n",
            "|    total_timesteps | 224000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.3    |\n",
            "|    critic_loss     | 0.0569   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 223000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 20.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 228      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1695     |\n",
            "|    total_timesteps | 228000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -26.3    |\n",
            "|    critic_loss     | 0.0515   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 227000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 22.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 232      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1725     |\n",
            "|    total_timesteps | 232000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -26.6    |\n",
            "|    critic_loss     | 0.0471   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 231000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 24.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 236      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1754     |\n",
            "|    total_timesteps | 236000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -26.9    |\n",
            "|    critic_loss     | 0.0509   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 235000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 25.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 240      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1783     |\n",
            "|    total_timesteps | 240000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -26.8    |\n",
            "|    critic_loss     | 0.0516   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 239000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 27.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 244      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1813     |\n",
            "|    total_timesteps | 244000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -26.5    |\n",
            "|    critic_loss     | 0.0452   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 243000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 28.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 248      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1842     |\n",
            "|    total_timesteps | 248000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.8    |\n",
            "|    critic_loss     | 0.0402   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 247000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 252      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1871     |\n",
            "|    total_timesteps | 252000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.4    |\n",
            "|    critic_loss     | 0.0416   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 251000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 30.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 256      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1901     |\n",
            "|    total_timesteps | 256000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.4    |\n",
            "|    critic_loss     | 0.0436   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 255000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 31       |\n",
            "| time/              |          |\n",
            "|    episodes        | 260      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1930     |\n",
            "|    total_timesteps | 260000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.2    |\n",
            "|    critic_loss     | 0.0421   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 259000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 30.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 264      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1960     |\n",
            "|    total_timesteps | 264000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.1    |\n",
            "|    critic_loss     | 0.047    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 263000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 268      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 1989     |\n",
            "|    total_timesteps | 268000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24.4    |\n",
            "|    critic_loss     | 0.0459   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 267000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 30.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 272      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2018     |\n",
            "|    total_timesteps | 272000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.3    |\n",
            "|    critic_loss     | 0.0499   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 271000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.8     |\n",
            "| time/              |          |\n",
            "|    episodes        | 276      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2048     |\n",
            "|    total_timesteps | 276000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -22.2    |\n",
            "|    critic_loss     | 0.0441   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 275000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 280      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2077     |\n",
            "|    total_timesteps | 280000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.9    |\n",
            "|    critic_loss     | 0.0461   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 279000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.1     |\n",
            "| time/              |          |\n",
            "|    episodes        | 284      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2107     |\n",
            "|    total_timesteps | 284000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.3    |\n",
            "|    critic_loss     | 0.0388   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 283000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 28.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 288      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2136     |\n",
            "|    total_timesteps | 288000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20      |\n",
            "|    critic_loss     | 0.0367   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 287000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 28.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 292      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2165     |\n",
            "|    total_timesteps | 292000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.6    |\n",
            "|    critic_loss     | 0.0343   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 291000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 27.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 296      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2195     |\n",
            "|    total_timesteps | 296000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.6    |\n",
            "|    critic_loss     | 0.0317   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 295000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 300      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2224     |\n",
            "|    total_timesteps | 300000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.1    |\n",
            "|    critic_loss     | 0.033    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 299000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 304      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2254     |\n",
            "|    total_timesteps | 304000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17.8    |\n",
            "|    critic_loss     | 0.0329   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 303000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 308      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2283     |\n",
            "|    total_timesteps | 308000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18      |\n",
            "|    critic_loss     | 0.0421   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 307000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 312      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2313     |\n",
            "|    total_timesteps | 312000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.6    |\n",
            "|    critic_loss     | 0.037    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 311000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 25.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 316      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2342     |\n",
            "|    total_timesteps | 316000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.5    |\n",
            "|    critic_loss     | 0.0357   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 315000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 25.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 320      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2372     |\n",
            "|    total_timesteps | 320000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18      |\n",
            "|    critic_loss     | 0.0371   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 319000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 25.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 324      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2401     |\n",
            "|    total_timesteps | 324000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17.5    |\n",
            "|    critic_loss     | 0.049    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 323000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 25.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 328      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2431     |\n",
            "|    total_timesteps | 328000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.6    |\n",
            "|    critic_loss     | 0.0581   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 327000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 25.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 332      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2460     |\n",
            "|    total_timesteps | 332000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.9    |\n",
            "|    critic_loss     | 0.1      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 331000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 336      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2490     |\n",
            "|    total_timesteps | 336000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.8    |\n",
            "|    critic_loss     | 0.0725   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 335000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 340      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2519     |\n",
            "|    total_timesteps | 340000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.7    |\n",
            "|    critic_loss     | 0.0607   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 339000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 27.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 344      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2549     |\n",
            "|    total_timesteps | 344000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.9    |\n",
            "|    critic_loss     | 0.0623   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 343000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 27.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 348      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2578     |\n",
            "|    total_timesteps | 348000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.9    |\n",
            "|    critic_loss     | 0.0612   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 347000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.1     |\n",
            "| time/              |          |\n",
            "|    episodes        | 352      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2608     |\n",
            "|    total_timesteps | 352000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.1    |\n",
            "|    critic_loss     | 0.0602   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 351000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 356      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2637     |\n",
            "|    total_timesteps | 356000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.2    |\n",
            "|    critic_loss     | 0.0666   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 355000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 30.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 360      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2667     |\n",
            "|    total_timesteps | 360000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.6    |\n",
            "|    critic_loss     | 0.0919   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 359000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 30.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 364      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2696     |\n",
            "|    total_timesteps | 364000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.9    |\n",
            "|    critic_loss     | 0.109    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 363000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 31.1     |\n",
            "| time/              |          |\n",
            "|    episodes        | 368      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2726     |\n",
            "|    total_timesteps | 368000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.4    |\n",
            "|    critic_loss     | 0.113    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 367000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.8     |\n",
            "| time/              |          |\n",
            "|    episodes        | 372      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2755     |\n",
            "|    total_timesteps | 372000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.8    |\n",
            "|    critic_loss     | 0.114    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 371000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 376      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 2785     |\n",
            "|    total_timesteps | 376000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.7    |\n",
            "|    critic_loss     | 0.102    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 375000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 380      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 2814     |\n",
            "|    total_timesteps | 380000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.9    |\n",
            "|    critic_loss     | 0.0911   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 379000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 28.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 384      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 2844     |\n",
            "|    total_timesteps | 384000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19      |\n",
            "|    critic_loss     | 0.0865   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 383000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 388      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 2873     |\n",
            "|    total_timesteps | 388000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.8    |\n",
            "|    critic_loss     | 0.0872   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 387000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 392      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 2903     |\n",
            "|    total_timesteps | 392000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.5    |\n",
            "|    critic_loss     | 0.0839   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 391000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 28.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 396      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 2932     |\n",
            "|    total_timesteps | 396000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.1    |\n",
            "|    critic_loss     | 0.0775   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 395000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 27.8     |\n",
            "| time/              |          |\n",
            "|    episodes        | 400      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 2962     |\n",
            "|    total_timesteps | 400000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17.5    |\n",
            "|    critic_loss     | 0.0772   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 399000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 404      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 2991     |\n",
            "|    total_timesteps | 404000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17.3    |\n",
            "|    critic_loss     | 0.0744   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 403000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 408      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3021     |\n",
            "|    total_timesteps | 408000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -16.7    |\n",
            "|    critic_loss     | 0.0706   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 407000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 26.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 412      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3051     |\n",
            "|    total_timesteps | 412000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -16      |\n",
            "|    critic_loss     | 0.0665   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 411000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 27.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 416      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3080     |\n",
            "|    total_timesteps | 416000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -15.4    |\n",
            "|    critic_loss     | 0.0602   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 415000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 27.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 420      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3110     |\n",
            "|    total_timesteps | 420000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -14.9    |\n",
            "|    critic_loss     | 0.0534   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 419000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 28       |\n",
            "| time/              |          |\n",
            "|    episodes        | 424      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3140     |\n",
            "|    total_timesteps | 424000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -14.6    |\n",
            "|    critic_loss     | 0.0488   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 423000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 28.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 428      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3169     |\n",
            "|    total_timesteps | 428000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -14      |\n",
            "|    critic_loss     | 0.0431   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 427000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 432      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3199     |\n",
            "|    total_timesteps | 432000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -13.4    |\n",
            "|    critic_loss     | 0.0394   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 431000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 436      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3229     |\n",
            "|    total_timesteps | 436000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -13.1    |\n",
            "|    critic_loss     | 0.0387   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 435000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 440      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3258     |\n",
            "|    total_timesteps | 440000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.9    |\n",
            "|    critic_loss     | 0.0416   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 439000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 444      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3288     |\n",
            "|    total_timesteps | 444000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.7    |\n",
            "|    critic_loss     | 0.0425   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 443000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 448      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3318     |\n",
            "|    total_timesteps | 448000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.5    |\n",
            "|    critic_loss     | 0.0369   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 447000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 452      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3347     |\n",
            "|    total_timesteps | 452000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.4    |\n",
            "|    critic_loss     | 0.0359   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 451000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 456      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3377     |\n",
            "|    total_timesteps | 456000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.1    |\n",
            "|    critic_loss     | 0.0329   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 455000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 29.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 460      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3407     |\n",
            "|    total_timesteps | 460000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.9    |\n",
            "|    critic_loss     | 0.0344   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 459000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 30.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 464      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3436     |\n",
            "|    total_timesteps | 464000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.7    |\n",
            "|    critic_loss     | 0.0318   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 463000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 31       |\n",
            "| time/              |          |\n",
            "|    episodes        | 468      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3466     |\n",
            "|    total_timesteps | 468000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.6    |\n",
            "|    critic_loss     | 0.0319   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 467000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 32.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 472      |\n",
            "|    fps             | 135      |\n",
            "|    time_elapsed    | 3496     |\n",
            "|    total_timesteps | 472000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.5    |\n",
            "|    critic_loss     | 0.0316   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 471000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 33.1     |\n",
            "| time/              |          |\n",
            "|    episodes        | 476      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 3526     |\n",
            "|    total_timesteps | 476000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.6    |\n",
            "|    critic_loss     | 0.0307   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 475000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 33.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 480      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 3555     |\n",
            "|    total_timesteps | 480000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.4    |\n",
            "|    critic_loss     | 0.0288   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 479000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 34.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 484      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 3585     |\n",
            "|    total_timesteps | 484000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.5    |\n",
            "|    critic_loss     | 0.0261   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 483000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 35       |\n",
            "| time/              |          |\n",
            "|    episodes        | 488      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 3615     |\n",
            "|    total_timesteps | 488000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.2    |\n",
            "|    critic_loss     | 0.0282   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 487000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 35.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 492      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 3644     |\n",
            "|    total_timesteps | 492000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.1    |\n",
            "|    critic_loss     | 0.0293   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 491000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 37.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 496      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 3674     |\n",
            "|    total_timesteps | 496000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11      |\n",
            "|    critic_loss     | 0.0272   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 495000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 38.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 500      |\n",
            "|    fps             | 134      |\n",
            "|    time_elapsed    | 3704     |\n",
            "|    total_timesteps | 500000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11      |\n",
            "|    critic_loss     | 0.027    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 499000   |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import TD3\n",
        "\n",
        "model = TD3(\"MlpPolicy\", \"Swimmer-v3\", verbose=1).learn(500_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjijWCY6HqaC"
      },
      "outputs": [],
      "source": [
        "# Now, let's make an implimentation ourselves!\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\tdef add(self, state, action, next_state, reward, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.not_done[self.ptr] = 1. - done\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "\t\t)\n",
        "\n",
        "# ToDo:\n",
        "\n",
        "# Make Actor and Critic Networks\n",
        "# Set up Environment\n",
        "# Set up policy\n",
        "# Set up replay buffer\n",
        "# Make eval policy function\n",
        "\n",
        "# Implimentations: HW4 Code, https://github.com/sfujim/TD3/blob/master/main.py, https://github.com/FranciscoHu17/BipedalWalker/blob/main/TD3/td3.py\n",
        "# Other links: https://arxiv.org/abs/1802.09477 (TD3 Paper), \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim, max_action):\n",
        "\t\tsuper(Actor, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, action_dim)\n",
        "\t\t\n",
        "\t\tself.max_action = max_action\n",
        "\t\t\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\ta = F.relu(self.l1(state))\n",
        "\t\ta = F.relu(self.l2(a))\n",
        "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\t# Q1 architecture\n",
        "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, 1)\n",
        "\n",
        "\t\t# Q2 architecture\n",
        "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l5 = nn.Linear(256, 256)\n",
        "\t\tself.l6 = nn.Linear(256, 1)\n",
        "\n",
        "\n",
        "\tdef forward(self, state, action):\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\n",
        "\t\tq2 = F.relu(self.l4(sa))\n",
        "\t\tq2 = F.relu(self.l5(q2))\n",
        "\t\tq2 = self.l6(q2)\n",
        "\t\treturn q1, q2\n",
        "\n",
        "\n",
        "\tdef Q1(self, state, action):\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\t\treturn q1"
      ],
      "metadata": {
        "id": "YIN0PKWyUqMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3(object):\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tstate_dim,\n",
        "\t\taction_dim,\n",
        "\t\tmax_action,\n",
        "\t\tdiscount=0.99,\n",
        "\t\ttau=0.005,\n",
        "\t\tpolicy_noise=0.2,\n",
        "\t\tnoise_clip=0.5,\n",
        "\t\tpolicy_freq=2\n",
        "\t):\n",
        "\n",
        "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\t\tself.policy_noise = policy_noise\n",
        "\t\tself.noise_clip = noise_clip\n",
        "\t\tself.policy_freq = policy_freq\n",
        "\n",
        "\t\tself.total_it = 0\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\t  state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\t  return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=256):\n",
        "\t\tself.total_it += 1\n",
        "\n",
        "\t\t# Sample replay buffer \n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# Select action according to policy and add clipped noise\n",
        "\t\t\tnoise = (\n",
        "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
        "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
        "\t\t\t\n",
        "\t\t\tnext_action = (\n",
        "\t\t\t\tself.actor_target(next_state) + noise\n",
        "\t\t\t).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "\t\t\t# Compute the target Q value\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
        "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
        "\n",
        "\t\t# Get current Q estimates\n",
        "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Delayed policy updates\n",
        "\t\tif self.total_it % self.policy_freq == 0:\n",
        "\n",
        "\t\t\t# Compute actor losse\n",
        "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\t\t\t\n",
        "\t\t\t# Optimize the actor \n",
        "\t\t\tself.actor_optimizer.zero_grad()\n",
        "\t\t\tactor_loss.backward()\n",
        "\t\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t\t# Update the frozen target models\n",
        "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\t\t"
      ],
      "metadata": {
        "id": "70cExEFiUrOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_policy(policy, env_name, seed, eval_episodes=5):\n",
        "\teval_env = gym.make(env_name)\n",
        "\teval_env.seed(seed + 100)\n",
        "\n",
        "\tavg_reward = 0.\n",
        "\tfor _ in range(eval_episodes):\n",
        "\t\tstate, done = eval_env.reset(), False\n",
        "\t\twhile not done:\n",
        "\t\t\taction = policy.select_action(np.array(state))\n",
        "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
        "\t\t\tavg_reward += reward\n",
        "\n",
        "\tavg_reward /= eval_episodes\n",
        "\n",
        "\tprint(\"---------------------------------------\")\n",
        "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "\tprint(\"---------------------------------------\")\n",
        "\treturn avg_reward"
      ],
      "metadata": {
        "id": "DbbvXIt4Uvss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Swimmer-v3\")\n",
        "\n",
        "# Set seeds\n",
        "seed = 1\n",
        "start_timesteps = 15e3\n",
        "max_timesteps = 2e5\n",
        "expl_noise = 0.3\n",
        "batch_size = 256\n",
        "eval_freq = 5e3\n",
        "\n",
        "env.seed(3)\n",
        "env.action_space.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0] \n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "print(state_dim, action_dim, max_action)\n",
        "\n",
        "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "evaluations = [eval_policy(policy, \"Swimmer-v3\", seed)]\n",
        "print(evaluations)\n",
        "\n",
        "state, done = env.reset(), False\n",
        "episode_reward = 0\n",
        "episode_timesteps = 0\n",
        "episode_num = 0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppDiNg4mU5XC",
        "outputId": "6ba21fa4-e0ff-4e60-9f20-d1d3f3aaec21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 2 1.0\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 25.131\n",
            "---------------------------------------\n",
            "[25.130868969325824]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(int(max_timesteps)):\n",
        "    \n",
        "    episode_timesteps += 1\n",
        "\n",
        "    # Select action randomly or according to policy\n",
        "    if t < start_timesteps:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = (\n",
        "            policy.select_action(np.array(state))\n",
        "            + np.random.normal(0, max_action * expl_noise, size=action_dim)\n",
        "        ).clip(-max_action, max_action)\n",
        "\n",
        "    # Perform action\n",
        "    next_state, reward, done, _ = env.step(action) \n",
        "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "    # Store data in replay buffer\n",
        "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "\n",
        "    # Train agent after collecting sufficient data\n",
        "    if t >= start_timesteps:\n",
        "        policy.train(replay_buffer, batch_size)\n",
        "\n",
        "    if done: \n",
        "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
        "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "        # Reset environment\n",
        "        state, done = env.reset(), False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1 \n",
        "\n",
        "    # Evaluate episode\n",
        "    if (t + 1) % eval_freq == 0:\n",
        "        rrr = eval_policy(policy, \"Swimmer-v3\", seed)\n",
        "\n",
        "        evaluations.append(rrr)\n",
        "        # print(evaluations)\n",
        "        # np.save(f\"./results/{file_name}\", evaluations)\n",
        "        # if args.save_model: policy.save(f\"./models/{file_name}\")\n",
        "\n",
        "print(evaluations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg3FUnyab04O",
        "outputId": "62b57a86-c16a-472a-f52e-7d91b6ad81e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total T: 1000 Episode Num: 1 Episode T: 1000 Reward: -13.572\n",
            "Total T: 2000 Episode Num: 2 Episode T: 1000 Reward: -1.929\n",
            "Total T: 3000 Episode Num: 3 Episode T: 1000 Reward: 2.320\n",
            "Total T: 4000 Episode Num: 4 Episode T: 1000 Reward: -3.102\n",
            "Total T: 5000 Episode Num: 5 Episode T: 1000 Reward: 10.907\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 25.131\n",
            "---------------------------------------\n",
            "Total T: 6000 Episode Num: 6 Episode T: 1000 Reward: 14.276\n",
            "Total T: 7000 Episode Num: 7 Episode T: 1000 Reward: -11.774\n",
            "Total T: 8000 Episode Num: 8 Episode T: 1000 Reward: -11.882\n",
            "Total T: 9000 Episode Num: 9 Episode T: 1000 Reward: -5.698\n",
            "Total T: 10000 Episode Num: 10 Episode T: 1000 Reward: 14.078\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 25.131\n",
            "---------------------------------------\n",
            "Total T: 11000 Episode Num: 11 Episode T: 1000 Reward: -4.490\n",
            "Total T: 12000 Episode Num: 12 Episode T: 1000 Reward: 6.028\n",
            "Total T: 13000 Episode Num: 13 Episode T: 1000 Reward: 6.257\n",
            "Total T: 14000 Episode Num: 14 Episode T: 1000 Reward: -13.670\n",
            "Total T: 15000 Episode Num: 15 Episode T: 1000 Reward: 17.456\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 25.131\n",
            "---------------------------------------\n",
            "Total T: 16000 Episode Num: 16 Episode T: 1000 Reward: 11.525\n",
            "Total T: 17000 Episode Num: 17 Episode T: 1000 Reward: 40.952\n",
            "Total T: 18000 Episode Num: 18 Episode T: 1000 Reward: 31.845\n",
            "Total T: 19000 Episode Num: 19 Episode T: 1000 Reward: 27.052\n",
            "Total T: 20000 Episode Num: 20 Episode T: 1000 Reward: 20.751\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 21.405\n",
            "---------------------------------------\n",
            "Total T: 21000 Episode Num: 21 Episode T: 1000 Reward: 0.287\n",
            "Total T: 22000 Episode Num: 22 Episode T: 1000 Reward: 21.523\n",
            "Total T: 23000 Episode Num: 23 Episode T: 1000 Reward: 20.215\n",
            "Total T: 24000 Episode Num: 24 Episode T: 1000 Reward: 21.770\n",
            "Total T: 25000 Episode Num: 25 Episode T: 1000 Reward: 26.087\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 31.261\n",
            "---------------------------------------\n",
            "Total T: 26000 Episode Num: 26 Episode T: 1000 Reward: 32.121\n",
            "Total T: 27000 Episode Num: 27 Episode T: 1000 Reward: 25.940\n",
            "Total T: 28000 Episode Num: 28 Episode T: 1000 Reward: -5.931\n",
            "Total T: 29000 Episode Num: 29 Episode T: 1000 Reward: 43.422\n",
            "Total T: 30000 Episode Num: 30 Episode T: 1000 Reward: 19.534\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 35.517\n",
            "---------------------------------------\n",
            "Total T: 31000 Episode Num: 31 Episode T: 1000 Reward: 29.177\n",
            "Total T: 32000 Episode Num: 32 Episode T: 1000 Reward: 21.093\n",
            "Total T: 33000 Episode Num: 33 Episode T: 1000 Reward: 35.035\n",
            "Total T: 34000 Episode Num: 34 Episode T: 1000 Reward: 38.846\n",
            "Total T: 35000 Episode Num: 35 Episode T: 1000 Reward: 35.664\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 32.182\n",
            "---------------------------------------\n",
            "Total T: 36000 Episode Num: 36 Episode T: 1000 Reward: 33.481\n",
            "Total T: 37000 Episode Num: 37 Episode T: 1000 Reward: 29.850\n",
            "Total T: 38000 Episode Num: 38 Episode T: 1000 Reward: 30.404\n",
            "Total T: 39000 Episode Num: 39 Episode T: 1000 Reward: 22.492\n",
            "Total T: 40000 Episode Num: 40 Episode T: 1000 Reward: 26.361\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 33.655\n",
            "---------------------------------------\n",
            "Total T: 41000 Episode Num: 41 Episode T: 1000 Reward: 24.526\n",
            "Total T: 42000 Episode Num: 42 Episode T: 1000 Reward: 24.364\n",
            "Total T: 43000 Episode Num: 43 Episode T: 1000 Reward: 26.529\n",
            "Total T: 44000 Episode Num: 44 Episode T: 1000 Reward: 23.454\n",
            "Total T: 45000 Episode Num: 45 Episode T: 1000 Reward: 34.239\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 38.980\n",
            "---------------------------------------\n",
            "Total T: 46000 Episode Num: 46 Episode T: 1000 Reward: 26.775\n",
            "Total T: 47000 Episode Num: 47 Episode T: 1000 Reward: 24.653\n",
            "Total T: 48000 Episode Num: 48 Episode T: 1000 Reward: 35.339\n",
            "Total T: 49000 Episode Num: 49 Episode T: 1000 Reward: 24.748\n",
            "Total T: 50000 Episode Num: 50 Episode T: 1000 Reward: 48.258\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 23.666\n",
            "---------------------------------------\n",
            "Total T: 51000 Episode Num: 51 Episode T: 1000 Reward: 32.670\n",
            "Total T: 52000 Episode Num: 52 Episode T: 1000 Reward: 35.152\n",
            "Total T: 53000 Episode Num: 53 Episode T: 1000 Reward: 37.263\n",
            "Total T: 54000 Episode Num: 54 Episode T: 1000 Reward: 32.841\n",
            "Total T: 55000 Episode Num: 55 Episode T: 1000 Reward: 30.522\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 42.904\n",
            "---------------------------------------\n",
            "Total T: 56000 Episode Num: 56 Episode T: 1000 Reward: 40.542\n",
            "Total T: 57000 Episode Num: 57 Episode T: 1000 Reward: 48.092\n",
            "Total T: 58000 Episode Num: 58 Episode T: 1000 Reward: 45.803\n",
            "Total T: 59000 Episode Num: 59 Episode T: 1000 Reward: 36.978\n",
            "Total T: 60000 Episode Num: 60 Episode T: 1000 Reward: 41.489\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 33.374\n",
            "---------------------------------------\n",
            "Total T: 61000 Episode Num: 61 Episode T: 1000 Reward: 31.849\n",
            "Total T: 62000 Episode Num: 62 Episode T: 1000 Reward: 34.504\n",
            "Total T: 63000 Episode Num: 63 Episode T: 1000 Reward: 33.524\n",
            "Total T: 64000 Episode Num: 64 Episode T: 1000 Reward: 34.026\n",
            "Total T: 65000 Episode Num: 65 Episode T: 1000 Reward: 25.645\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 28.548\n",
            "---------------------------------------\n",
            "Total T: 66000 Episode Num: 66 Episode T: 1000 Reward: 29.139\n",
            "Total T: 67000 Episode Num: 67 Episode T: 1000 Reward: 31.478\n",
            "Total T: 68000 Episode Num: 68 Episode T: 1000 Reward: 32.023\n",
            "Total T: 69000 Episode Num: 69 Episode T: 1000 Reward: 28.312\n",
            "Total T: 70000 Episode Num: 70 Episode T: 1000 Reward: 39.079\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 40.490\n",
            "---------------------------------------\n",
            "Total T: 71000 Episode Num: 71 Episode T: 1000 Reward: 43.117\n",
            "Total T: 72000 Episode Num: 72 Episode T: 1000 Reward: 26.086\n",
            "Total T: 73000 Episode Num: 73 Episode T: 1000 Reward: 18.952\n",
            "Total T: 74000 Episode Num: 74 Episode T: 1000 Reward: 30.444\n",
            "Total T: 75000 Episode Num: 75 Episode T: 1000 Reward: 35.189\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 31.563\n",
            "---------------------------------------\n",
            "Total T: 76000 Episode Num: 76 Episode T: 1000 Reward: 29.551\n",
            "Total T: 77000 Episode Num: 77 Episode T: 1000 Reward: 34.942\n",
            "Total T: 78000 Episode Num: 78 Episode T: 1000 Reward: 33.961\n",
            "Total T: 79000 Episode Num: 79 Episode T: 1000 Reward: 35.275\n",
            "Total T: 80000 Episode Num: 80 Episode T: 1000 Reward: 32.693\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 34.953\n",
            "---------------------------------------\n",
            "Total T: 81000 Episode Num: 81 Episode T: 1000 Reward: 22.619\n",
            "Total T: 82000 Episode Num: 82 Episode T: 1000 Reward: 28.460\n",
            "Total T: 83000 Episode Num: 83 Episode T: 1000 Reward: 34.596\n",
            "Total T: 84000 Episode Num: 84 Episode T: 1000 Reward: 41.606\n",
            "Total T: 85000 Episode Num: 85 Episode T: 1000 Reward: 39.469\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 20.005\n",
            "---------------------------------------\n",
            "Total T: 86000 Episode Num: 86 Episode T: 1000 Reward: 21.589\n",
            "Total T: 87000 Episode Num: 87 Episode T: 1000 Reward: 30.752\n",
            "Total T: 88000 Episode Num: 88 Episode T: 1000 Reward: 38.194\n",
            "Total T: 89000 Episode Num: 89 Episode T: 1000 Reward: 39.983\n",
            "Total T: 90000 Episode Num: 90 Episode T: 1000 Reward: 33.377\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 46.542\n",
            "---------------------------------------\n",
            "Total T: 91000 Episode Num: 91 Episode T: 1000 Reward: 34.333\n",
            "Total T: 92000 Episode Num: 92 Episode T: 1000 Reward: 37.981\n",
            "Total T: 93000 Episode Num: 93 Episode T: 1000 Reward: 38.196\n",
            "Total T: 94000 Episode Num: 94 Episode T: 1000 Reward: 39.715\n",
            "Total T: 95000 Episode Num: 95 Episode T: 1000 Reward: 39.873\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 49.414\n",
            "---------------------------------------\n",
            "Total T: 96000 Episode Num: 96 Episode T: 1000 Reward: 40.163\n",
            "Total T: 97000 Episode Num: 97 Episode T: 1000 Reward: 39.304\n",
            "Total T: 98000 Episode Num: 98 Episode T: 1000 Reward: 43.097\n",
            "Total T: 99000 Episode Num: 99 Episode T: 1000 Reward: 44.385\n",
            "Total T: 100000 Episode Num: 100 Episode T: 1000 Reward: 39.219\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 32.894\n",
            "---------------------------------------\n",
            "Total T: 101000 Episode Num: 101 Episode T: 1000 Reward: 41.044\n",
            "Total T: 102000 Episode Num: 102 Episode T: 1000 Reward: 43.702\n",
            "Total T: 103000 Episode Num: 103 Episode T: 1000 Reward: 43.411\n",
            "Total T: 104000 Episode Num: 104 Episode T: 1000 Reward: 43.097\n",
            "Total T: 105000 Episode Num: 105 Episode T: 1000 Reward: 42.535\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 33.827\n",
            "---------------------------------------\n",
            "Total T: 106000 Episode Num: 106 Episode T: 1000 Reward: 42.427\n",
            "Total T: 107000 Episode Num: 107 Episode T: 1000 Reward: 41.045\n",
            "Total T: 108000 Episode Num: 108 Episode T: 1000 Reward: 18.314\n",
            "Total T: 109000 Episode Num: 109 Episode T: 1000 Reward: 20.734\n",
            "Total T: 110000 Episode Num: 110 Episode T: 1000 Reward: 6.051\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 6.847\n",
            "---------------------------------------\n",
            "Total T: 111000 Episode Num: 111 Episode T: 1000 Reward: 12.297\n",
            "Total T: 112000 Episode Num: 112 Episode T: 1000 Reward: 10.738\n",
            "Total T: 113000 Episode Num: 113 Episode T: 1000 Reward: 10.666\n",
            "Total T: 114000 Episode Num: 114 Episode T: 1000 Reward: 41.741\n",
            "Total T: 115000 Episode Num: 115 Episode T: 1000 Reward: 43.726\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 47.018\n",
            "---------------------------------------\n",
            "Total T: 116000 Episode Num: 116 Episode T: 1000 Reward: 39.790\n",
            "Total T: 117000 Episode Num: 117 Episode T: 1000 Reward: 42.462\n",
            "Total T: 118000 Episode Num: 118 Episode T: 1000 Reward: 43.500\n",
            "Total T: 119000 Episode Num: 119 Episode T: 1000 Reward: 47.555\n",
            "Total T: 120000 Episode Num: 120 Episode T: 1000 Reward: 46.612\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 39.573\n",
            "---------------------------------------\n",
            "Total T: 121000 Episode Num: 121 Episode T: 1000 Reward: 39.985\n",
            "Total T: 122000 Episode Num: 122 Episode T: 1000 Reward: 37.364\n",
            "Total T: 123000 Episode Num: 123 Episode T: 1000 Reward: 47.606\n",
            "Total T: 124000 Episode Num: 124 Episode T: 1000 Reward: 39.070\n",
            "Total T: 125000 Episode Num: 125 Episode T: 1000 Reward: 50.403\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 45.385\n",
            "---------------------------------------\n",
            "Total T: 126000 Episode Num: 126 Episode T: 1000 Reward: 40.274\n",
            "Total T: 127000 Episode Num: 127 Episode T: 1000 Reward: 39.945\n",
            "Total T: 128000 Episode Num: 128 Episode T: 1000 Reward: 49.038\n",
            "Total T: 129000 Episode Num: 129 Episode T: 1000 Reward: 49.907\n",
            "Total T: 130000 Episode Num: 130 Episode T: 1000 Reward: 46.766\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 46.197\n",
            "---------------------------------------\n",
            "Total T: 131000 Episode Num: 131 Episode T: 1000 Reward: 50.890\n",
            "Total T: 132000 Episode Num: 132 Episode T: 1000 Reward: 47.721\n",
            "Total T: 133000 Episode Num: 133 Episode T: 1000 Reward: 47.506\n",
            "Total T: 134000 Episode Num: 134 Episode T: 1000 Reward: 42.622\n",
            "Total T: 135000 Episode Num: 135 Episode T: 1000 Reward: 47.280\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 40.086\n",
            "---------------------------------------\n",
            "Total T: 136000 Episode Num: 136 Episode T: 1000 Reward: 37.689\n",
            "Total T: 137000 Episode Num: 137 Episode T: 1000 Reward: 36.065\n",
            "Total T: 138000 Episode Num: 138 Episode T: 1000 Reward: 52.622\n",
            "Total T: 139000 Episode Num: 139 Episode T: 1000 Reward: 38.119\n",
            "Total T: 140000 Episode Num: 140 Episode T: 1000 Reward: 42.512\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 47.354\n",
            "---------------------------------------\n",
            "Total T: 141000 Episode Num: 141 Episode T: 1000 Reward: 40.676\n",
            "Total T: 142000 Episode Num: 142 Episode T: 1000 Reward: 44.646\n",
            "Total T: 143000 Episode Num: 143 Episode T: 1000 Reward: 47.800\n",
            "Total T: 144000 Episode Num: 144 Episode T: 1000 Reward: 2.122\n",
            "Total T: 145000 Episode Num: 145 Episode T: 1000 Reward: 49.987\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 36.094\n",
            "---------------------------------------\n",
            "Total T: 146000 Episode Num: 146 Episode T: 1000 Reward: 46.652\n",
            "Total T: 147000 Episode Num: 147 Episode T: 1000 Reward: 47.146\n",
            "Total T: 148000 Episode Num: 148 Episode T: 1000 Reward: 30.261\n",
            "Total T: 149000 Episode Num: 149 Episode T: 1000 Reward: 35.800\n",
            "Total T: 150000 Episode Num: 150 Episode T: 1000 Reward: 38.437\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 46.789\n",
            "---------------------------------------\n",
            "Total T: 151000 Episode Num: 151 Episode T: 1000 Reward: 47.337\n",
            "Total T: 152000 Episode Num: 152 Episode T: 1000 Reward: 36.586\n",
            "Total T: 153000 Episode Num: 153 Episode T: 1000 Reward: 48.030\n",
            "Total T: 154000 Episode Num: 154 Episode T: 1000 Reward: 49.031\n",
            "Total T: 155000 Episode Num: 155 Episode T: 1000 Reward: 36.185\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 45.875\n",
            "---------------------------------------\n",
            "Total T: 156000 Episode Num: 156 Episode T: 1000 Reward: 52.399\n",
            "Total T: 157000 Episode Num: 157 Episode T: 1000 Reward: 42.656\n",
            "Total T: 158000 Episode Num: 158 Episode T: 1000 Reward: 54.265\n",
            "Total T: 159000 Episode Num: 159 Episode T: 1000 Reward: 34.282\n",
            "Total T: 160000 Episode Num: 160 Episode T: 1000 Reward: 48.570\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 50.129\n",
            "---------------------------------------\n",
            "Total T: 161000 Episode Num: 161 Episode T: 1000 Reward: 50.182\n",
            "Total T: 162000 Episode Num: 162 Episode T: 1000 Reward: 40.971\n",
            "Total T: 163000 Episode Num: 163 Episode T: 1000 Reward: 43.992\n",
            "Total T: 164000 Episode Num: 164 Episode T: 1000 Reward: 45.200\n",
            "Total T: 165000 Episode Num: 165 Episode T: 1000 Reward: 37.920\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 46.689\n",
            "---------------------------------------\n",
            "Total T: 166000 Episode Num: 166 Episode T: 1000 Reward: 38.479\n",
            "Total T: 167000 Episode Num: 167 Episode T: 1000 Reward: 45.811\n",
            "Total T: 168000 Episode Num: 168 Episode T: 1000 Reward: 43.634\n",
            "Total T: 169000 Episode Num: 169 Episode T: 1000 Reward: 57.447\n",
            "Total T: 170000 Episode Num: 170 Episode T: 1000 Reward: 39.058\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 41.630\n",
            "---------------------------------------\n",
            "Total T: 171000 Episode Num: 171 Episode T: 1000 Reward: 49.449\n",
            "Total T: 172000 Episode Num: 172 Episode T: 1000 Reward: 52.505\n",
            "Total T: 173000 Episode Num: 173 Episode T: 1000 Reward: 39.728\n",
            "Total T: 174000 Episode Num: 174 Episode T: 1000 Reward: 44.971\n",
            "Total T: 175000 Episode Num: 175 Episode T: 1000 Reward: 43.906\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 41.697\n",
            "---------------------------------------\n",
            "Total T: 176000 Episode Num: 176 Episode T: 1000 Reward: 36.136\n",
            "Total T: 177000 Episode Num: 177 Episode T: 1000 Reward: 45.937\n",
            "Total T: 178000 Episode Num: 178 Episode T: 1000 Reward: 47.324\n",
            "Total T: 179000 Episode Num: 179 Episode T: 1000 Reward: 44.395\n",
            "Total T: 180000 Episode Num: 180 Episode T: 1000 Reward: 49.190\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 54.236\n",
            "---------------------------------------\n",
            "Total T: 181000 Episode Num: 181 Episode T: 1000 Reward: 41.660\n",
            "Total T: 182000 Episode Num: 182 Episode T: 1000 Reward: 58.170\n",
            "Total T: 183000 Episode Num: 183 Episode T: 1000 Reward: 52.656\n",
            "Total T: 184000 Episode Num: 184 Episode T: 1000 Reward: 44.489\n",
            "Total T: 185000 Episode Num: 185 Episode T: 1000 Reward: 50.067\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 57.147\n",
            "---------------------------------------\n",
            "Total T: 186000 Episode Num: 186 Episode T: 1000 Reward: 61.869\n",
            "Total T: 187000 Episode Num: 187 Episode T: 1000 Reward: 54.450\n",
            "Total T: 188000 Episode Num: 188 Episode T: 1000 Reward: 44.535\n",
            "Total T: 189000 Episode Num: 189 Episode T: 1000 Reward: 68.701\n",
            "Total T: 190000 Episode Num: 190 Episode T: 1000 Reward: 62.006\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 51.619\n",
            "---------------------------------------\n",
            "Total T: 191000 Episode Num: 191 Episode T: 1000 Reward: 53.571\n",
            "Total T: 192000 Episode Num: 192 Episode T: 1000 Reward: 57.009\n",
            "Total T: 193000 Episode Num: 193 Episode T: 1000 Reward: 59.816\n",
            "Total T: 194000 Episode Num: 194 Episode T: 1000 Reward: 41.269\n",
            "Total T: 195000 Episode Num: 195 Episode T: 1000 Reward: 39.491\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 46.842\n",
            "---------------------------------------\n",
            "Total T: 196000 Episode Num: 196 Episode T: 1000 Reward: 50.744\n",
            "Total T: 197000 Episode Num: 197 Episode T: 1000 Reward: 60.166\n",
            "Total T: 198000 Episode Num: 198 Episode T: 1000 Reward: 47.432\n",
            "Total T: 199000 Episode Num: 199 Episode T: 1000 Reward: 48.158\n",
            "Total T: 200000 Episode Num: 200 Episode T: 1000 Reward: 48.615\n",
            "---------------------------------------\n",
            "Evaluation over 5 episodes: 47.286\n",
            "---------------------------------------\n",
            "[25.130868969325824, 25.130868969325824, 25.130868969325824, 25.130868969325824, 21.404671300026745, 31.261115215053138, 35.517187127241534, 32.18236247855212, 33.654894633225894, 38.98008708297569, 23.666219726597134, 42.90356204129629, 33.37408336615637, 28.54762200771912, 40.489990836731465, 31.563464637595, 34.95297661770711, 20.00467788221384, 46.542146055881815, 49.41443486246153, 32.89380108068428, 33.82704340335378, 6.84698769828542, 47.01809116688987, 39.573066369941316, 45.38512349304314, 46.197241746952145, 40.08557279735491, 47.35394017722054, 36.09422805762801, 46.78872407176056, 45.875493922021285, 50.128739571370396, 46.688624739707585, 41.6299851747976, 41.69668839200976, 54.235658891906986, 57.146757676418346, 51.618767114703495, 46.842456729450085, 47.28576654379667]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}