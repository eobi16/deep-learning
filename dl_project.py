# -*- coding: utf-8 -*-
"""DL-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JwNZpVQMBg877blBxOZ0F-sMy7mGp5wj
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from tensorflow.keras.applications import EfficientNetB0
from keras.layers import Dropout, Dense, Flatten

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import os
import tensorflow as tf
import cv2
from tensorflow import keras
from tensorflow.keras import layers, Input
from tensorflow.keras.layers import Dense, InputLayer, Flatten
from tensorflow.keras.models import Sequential, Model
from  matplotlib import pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

from imblearn.over_sampling import SMOTE

from keras import Model

train_folder = '/content/gdrive/My Drive/Winter 2022/Deep Learning/lung _cancer_Dataset/train/'
val_folder = '/content/gdrive/My Drive/Winter 2022/Deep Learning/lung _cancer_Dataset/valid/'
test_folder = '/content/gdrive/My Drive/Winter 2022/Deep Learning/lung _cancer_Dataset/test/'

class_names = os.listdir(train_folder) # Get names of classes
class_name2id = { label: index for index, label in enumerate(class_names) } # Map class names to integer labels

class_names

# Directory with class pictures
for i in range(len(class_names)):
    locals()["class"+str(i)] = os.path.join(train_folder, class_names[i])

class_cnt = []
for k in range(len(class_names)):
  class_cnt.append(len(os.listdir(locals()["class"+str(k)])))
  print("Number of images in",class_names[k],":", len(os.listdir(locals()["class"+str(k)])))

sum(class_cnt)

# Directory with class pictures
for i in range(len(class_names)):
    locals()["val"+str(i)] = os.path.join(val_folder, class_names[i])

val_cnt = []
for k in range(len(class_names)):
  val_cnt.append(len(os.listdir(locals()["val"+str(k)])))
  print("Number of images in",class_names[k],":", len(os.listdir(locals()["val"+str(k)])))

sum(val_cnt)

# Directory with class pictures
for i in range(len(class_names)):
    locals()["test"+str(i)] = os.path.join(test_folder, class_names[i])

test_cnt = []
for k in range(len(class_names)):
  try:
    test_cnt.append(len(os.listdir(locals()["test"+str(k)])))
    print("Number of images in",class_names[k],":", len(os.listdir(locals()["test"+str(k)])))
  except:
    test_cnt.append(0)
    print("Number of images in",class_names[k],":", 0)

sum(test_cnt)

import matplotlib.pyplot as plt

def valuelabel(weight,students):
    for i in range(len(weight)):
        plt.text(i,students[i],students[i], ha = 'center',
                 bbox = dict(facecolor = 'cyan', alpha =0.8))


# making the bar chart on the data
plt.bar(class_names, class_cnt)

valuelabel(class_names, class_cnt) 
  
# giving title to the plot
plt.title("Distribution of the 12 classes in the training dataset")
  
# giving X and Y labels
plt.xlabel("Class Names")
plt.ylabel("Count of classes")

plt.xticks(rotation='vertical')

# visualizing the plot
plt.show()

# making the bar chart on the data
plt.bar(class_names, val_cnt)

valuelabel(class_names, val_cnt) 
  
# giving title to the plot
plt.title("Distribution of the 12 classes in the validation dataset")
  
# giving X and Y labels
plt.xlabel("Class Names")
plt.ylabel("Count of classes")

plt.xticks(rotation='vertical')

# visualizing the plot
plt.show()

# making the bar chart on the data
plt.bar(class_names, test_cnt)

valuelabel(class_names, test_cnt) 
  
# giving title to the plot
plt.title("Distribution of the 12 classes in the test dataset")
  
# giving X and Y labels
plt.xlabel("Class Names")
plt.ylabel("Count of classes")

plt.xticks(rotation='vertical')

# visualizing the plot
plt.show()

train_datagen = ImageDataGenerator(
        rescale = 1./255.,
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range = 40,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.2, # Randomly zoom image 
        shear_range = 0.2,
        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)
        horizontal_flip = True,  # randomly flip images
        vertical_flip=False)  # randomly flip images

batch_size = 128

train_generator = train_datagen.flow_from_directory(train_folder, batch_size = batch_size, classes = class_names, class_mode = 'categorical', target_size = (200, 200))

# All images will be rescaled by 1./255
test_datagen = ImageDataGenerator(rescale=1/255)

# Flow training images in batches of 128 using train_datagen generator
val_generator = test_datagen.flow_from_directory(
        val_folder,  # This is the source directory for training images
        target_size=(200, 200),  # All images will be resized to 200 x 200
        batch_size=batch_size,
        # Specify the classes explicitly
        classes = class_names,
        # Since we use categorical_crossentropy loss, we need categorical labels
        class_mode='categorical',
        shuffle=False)

# Flow training images in batches of 128 using train_datagen generator
test_generator = test_datagen.flow_from_directory(
        test_folder,  # This is the source directory for training images
        target_size=(200, 200),  # All images will be resized to 200 x 200
        batch_size=batch_size,
        # Specify the classes explicitly
        classes = class_names,
        # Since we use categorical_crossentropy loss, we need categorical labels
        class_mode='categorical',
        shuffle=False)



"""# **Method 1**"""

model = EfficientNetB0(input_shape = (224, 224, 3), include_top=False, weights='imagenet')

for layer in model.layers:
    layer.trainable = False

x = model.output
x = Flatten()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)

# Add a final sigmoid layer with 1 node for classification output
predictions = Dense(12, activation="softmax")(x)

model_final = Model(model.input, predictions)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)
model_final.compile(
    optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"]
)

eff_history = model_final.fit(train_generator, validation_data = val_generator, verbose = 2, epochs = 25)







"""# **Method 2**"""

from keras.utils.np_utils import to_categorical 
test_labels = test_generator.classes  
test_labels = to_categorical(test_labels, num_classes=len(class_names))

len(test_labels)

model = tf.keras.models.Sequential([
    # Note the input shape is the desired size of the image 200x 200 with 3 bytes color
    # The first convolution
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(200, 200, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The third convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fourth convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fifth convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # Flatten the results to feed into a dense layer
    tf.keras.layers.Flatten(),
    # 128 neuron in the fully-connected layer
    tf.keras.layers.Dense(128, activation='relu'),
    # 12 output neurons for 12 classes with the softmax activation
    tf.keras.layers.Dense(12, activation='softmax')
])
model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(lr=0.001),
              metrics=['acc'])

total_sample=train_generator.n

val_sample = val_generator.n

n_epochs = 30

from sklearn.utils import class_weight
import numpy as np

# class_weights = class_weight.compute_class_weight(
#            'balanced',
#             np.unique(train_generator.classes), 
#             train_generator.classes)

class_weights = class_weight.compute_class_weight(
                                        class_weight = "balanced",
                                        classes = np.unique(train_generator.classes),
                                        y = train_generator.classes                                                    
                                    )
class_weights = dict(zip(np.unique(train_generator.classes), class_weights))
class_weights

dict_classes = {
 'bbps-0-1': 0,
 'bbps-2-3': 1,
 'cecum': 2,
 'ulcerative-colitis-grade-2': 3,
 'dyed-resection-margins': 4,
 'pylorus': 5,
 'z-line': 6,
 'retroflex-stomach': 7,
 'esophagitis-a': 8,
 'retroflex-rectum': 9,
 'polyps': 10,
 'dyed-lifted-polyps': 11
}
dict_classes

train_generator.classes

total_sample

history = model.fit_generator(
        train_generator,
        steps_per_epoch=int(total_sample/batch_size),
        epochs=n_epochs,
        class_weight=class_weights,
        validation_data=val_generator,
        validation_steps=int(val_sample/batch_size),
        verbose=1)
model.save_weights('first_try.h5')  # always save your weights after training or during training

acc = history.history['acc']
val_acc = history.history['val_acc']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(n_epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

plt.figure(figsize=(7,4))
plt.plot([i+1 for i in range(n_epochs)],history.history['acc'],'-o',c='k',lw=2,markersize=9)
plt.grid(True)
plt.title("Training accuracy with epochs\n",fontsize=18)
plt.xlabel("Training epochs",fontsize=15)
plt.ylabel("Training accuracy",fontsize=15)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.show()

plt.figure(figsize=(7,4))
plt.plot([i+1 for i in range(n_epochs)],history.history['loss'],'-o',c='k',lw=2,markersize=9)
plt.grid(True)
plt.title("Training loss with epochs\n",fontsize=18)
plt.xlabel("Training epochs",fontsize=15)
plt.ylabel("Training loss",fontsize=15)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.show()

pred = np.round(model.predict(test_generator),0)

pred[0]

classification_metrics = metrics.classification_report(test_labels, pred)
print(classification_metrics)

import pandas as pd
import itertools

test_labels_tentative = pd.DataFrame(test_labels).idxmax(axis=1)
preds_tentative = pd.DataFrame(pred).idxmax(axis=1)

confusion_matrix = confusion_matrix(test_labels_tentative, preds_tentative)
confusion_matrix