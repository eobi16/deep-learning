# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vkXyWHuunDX1hoypHX6fXfIy8f_q9vNC

### Install and import relevant libraries
"""

pip install sentence_transformers

!python3 -m spacy download en_core_web_lg
# Restart runtime after this

import pandas as pd
import psycopg2
import nltk
import re
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import RegexpTokenizer
from nltk.stem.wordnet import WordNetLemmatizer

conn = psycopg2.connect(
    host="codd01.research.northwestern.edu",
    database="postgres",
    user="cpdbstudent",
    password="DataSci4AI")

"""## Load csv files"""

df = pd.read_sql_query("select * from data_officerallegation", con=conn)
narratives = pd.read_sql_query("select * from data_allegation", con=conn)

"""## 1. Merge tables"""

df.head()

narratives.head()

# Extract sustained and unsustained outcomes in two different dataframes
unsus_alleg = df[(df['final_finding'] == 'UN') | (df['final_finding'] == 'EX') | (df['final_finding'] == 'NS')]
sus_alleg = df[df['final_finding'] == 'SU']

# Merge the officer complaints with data_allegation to get complaint report linked to the allegation
unsus_data = pd.merge(unsus_alleg, narratives, left_on='allegation_id', right_on='crid')
sus_data = pd.merge(sus_alleg, narratives, left_on='allegation_id', right_on='crid')

# We are interested only in two columns so drop the rest
sus_data = sus_data[['allegation_id','cr_text']]
unsus_data = unsus_data[['allegation_id','cr_text']]

# Drop the rows with empty complaint report field
print(sus_data.shape)
sus_data = sus_data.dropna()
print(sus_data.shape)

# Drop the rows with empty complaint report field
print(unsus_data.shape)
unsus_data = unsus_data.dropna()
print(unsus_data.shape)

# Create a list of stop words from nltk
stop_words = set(stopwords.words("english"))
# print(sorted(stop_words))


# extend_list = ['alleged', 'missing', 'accused', 'alleges', 'reporting', 'party', 'officer', 'chicago', 'police', 'finding','none', 'entered','complainant', 'allegation', 'initial', 'intake', 'hour']
# for i in extend_list:
    # stop_words.add(i)

sus_data['cr_text'] = sus_data['cr_text'].apply(lambda x: str(x).replace('\n',' '))
unsus_data['cr_text'] = unsus_data['cr_text'].apply(lambda x: str(x).replace('\n',' '))

sus_data.iloc[1,1]

# Pre-process dataset to get a cleaned and normalised text corpus
def clean_text(dataset, datacol):
    corpus = []
    dataset['word_count'] = dataset[datacol].apply(lambda x: len(str(x).split(" ")))
    ds_count = len(dataset.word_count)
    for i in range(0, ds_count):
        
        # Remove punctuation
#         text = re.sub('[^a-zA-Z]', ' ', str(dataset[datacol][i]))

        # Remove special characters and digits
        text = re.sub("(\\d|\\W)+"," ",str(dataset[datacol].iloc[i]))

        # Convert to lowercase
        text = text.lower()

        # Remove tags
        text = re.sub("&lt;/?.*?&gt;"," &lt;&gt; ",text)
        
        # Convert to list from string
        text = text.split()

        # Stemming
        ps=PorterStemmer()

        # Lemmatisation
        lem = WordNetLemmatizer()
        text = [lem.lemmatize(word) for word in text if not word in  
                stop_words] 
        text = " ".join(text)
        text = text.replace('initial intake allegation ', '')
        text = text.replace('reporting party alleges accused officer ', '')
        corpus.append(text)
    
    return corpus, ds_count

unsus_corpus, unsus_ds_count = clean_text(unsus_data, 'cr_text')
sus_corpus, sus_ds_count = clean_text(sus_data, 'cr_text')

# Commented out IPython magic to ensure Python compatibility.
# Generate word cloud
from os import path
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
# %matplotlib inline
wordcloud_sus = WordCloud(
                          background_color='white',
                          stopwords=stop_words,
                          max_words=100,
                          max_font_size=50, 
                          random_state=42
                         ).generate(str(sus_corpus))
wordcloud_unsus = WordCloud(
                          background_color='white',
                          stopwords=stop_words,
                          max_words=100,
                          max_font_size=50, 
                          random_state=42
                         ).generate(str(unsus_corpus))
fig = plt.figure(1)
plt.imshow(wordcloud_sus)
plt.title('Sustained_allegations')
plt.axis('off')
fig = plt.figure(2)
plt.imshow(wordcloud_unsus)
plt.title('Unsustained_allegations')
plt.axis('off')
plt.show()

"""### Use BERT to extract keywords"""

# We use a separate cleaning function customised for BERT
def basic_clean(dataset, datacol):
    corpus = ""
    dataset['word_count'] = dataset[datacol].apply(lambda x: len(str(x).split(" ")))
    ds_count = len(dataset.word_count)
    for i in range(0, ds_count):
        
        text = re.sub("(\\d|\\W)+"," ",str(dataset[datacol].iloc[i]))

        # Convert to lowercase
        text = text.lower()

        # Remove tags
        text = re.sub("&lt;/?.*?&gt;"," &lt;&gt; ",text)

        text = text.split()
        text = [x for x in text if not x in stop_words]
        text = " ".join(text)
        corpus = corpus + ". " + text
    
    return "\"" + corpus + "\""

import warnings
warnings.filterwarnings('ignore')

from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re

model = SentenceTransformer('distilbert-base-nli-mean-tokens')
# sentences = "On October 27, an area of convection with a broad circulation persisted southeast of Guam, and slowly consolidated due to moderate wind shear."


with open('test_sentences.txt') as file:
   sentences = [line.strip() for line in file]

for sentence in sentences:
  cv_sus = CountVectorizer(max_features=10000, ngram_range=(2,2))
  X_sus = cv_sus.fit_transform([sentence])


  # Get feature names
  sus_feature_names=cv_sus.get_feature_names()

  # Sentence representations from SentenceTransformer
  sus_doc_embedding = model.encode([sentence])
  sus_candidate_embeddings = model.encode(sus_feature_names)

  top_n = 10
  distances = cosine_similarity(sus_doc_embedding, sus_candidate_embeddings)
  sus_keywords = [sus_feature_names[index] for index in distances.argsort()[0][-top_n:]]
  print('Test Sentence: ', sentence)
  print('Keywords are: ', sus_keywords)
  print('\n\n')

# Tokenize the text and build a vocabulary of known words
from sklearn.feature_extraction.text import CountVectorizer
import re

cv_sus = CountVectorizer(stop_words=stop_words, max_features=10000, ngram_range=(1,1))
X_sus = cv_sus.fit_transform([sus_uncleaned])

cv_unsus = CountVectorizer(stop_words=stop_words, max_features=10000, ngram_range=(1,1))
X_unsus = cv_unsus.fit_transform([unsus_uncleaned])

# Get feature names
sus_feature_names=cv_sus.get_feature_names()
unsus_feature_names=cv_unsus.get_feature_names()

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('distilbert-base-nli-mean-tokens')
unsus_doc_embedding = model.encode([unsus_uncleaned])
unsus_candidate_embeddings = model.encode(unsus_feature_names)

sus_doc_embedding = model.encode([sus_uncleaned])
sus_candidate_embeddings = model.encode(sus_feature_names)

from sklearn.metrics.pairwise import cosine_similarity

top_n = 10
distances = cosine_similarity(unsus_doc_embedding, unsus_candidate_embeddings)
unsus_keywords = [unsus_feature_names[index] for index in distances.argsort()[0][-top_n:]]

distances = cosine_similarity(sus_doc_embedding, sus_candidate_embeddings)
sus_keywords = [sus_feature_names[index] for index in distances.argsort()[0][-top_n:]]

print(sus_keywords)

print(unsus_keywords)

list(cv_sus.vocabulary_.keys())[:15]

list(cv_unsus.vocabulary_.keys())[:15]

# View most frequently occuring keywords
def get_top_n_words(corpus, n=None):
    vec = CountVectorizer().fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in      
                   vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], 
                       reverse=True)
    return words_freq[:n]

# Convert most freq words to dataframe for plotting bar plot
top_words_sus = get_top_n_words(sus_corpus, n=20)
top_df_sus = pd.DataFrame(top_words_sus)
top_df_sus.columns=["Keyword", "Frequency"]
print(top_df_sus)

top_words_unsus = get_top_n_words(unsus_corpus, n=20)
top_df_unsus = pd.DataFrame(top_words_unsus)
top_df_unsus.columns=["Keyword", "Frequency"]
print(top_df_unsus)

# Barplot of most freq words
import seaborn as sns
sns.set(rc={'figure.figsize':(13,8)})
fig = plt.figure(1)
g_sus = sns.barplot(x="Keyword", y="Frequency", data=top_df_sus, palette="Blues_d")
g_sus.set_xticklabels(g_sus.get_xticklabels(), rotation=45)
fig = plt.figure(2)
g_unsus = sns.barplot(x="Keyword", y="Frequency", data=top_df_unsus, palette="Reds_d")
g_unsus.set_xticklabels(g_unsus.get_xticklabels(), rotation=45)

# Most frequently occuring bigrams
def get_top_n2_words(corpus, n=None):
    vec1 = CountVectorizer(ngram_range=(2,2),  
            max_features=2000).fit(corpus)
    bag_of_words = vec1.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in     
                  vec1.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], 
                reverse=True)
    return words_freq[:n]

# Convert most freq bigrams to dataframe for plotting bar plot
top2_words_sus = get_top_n2_words(sus_corpus, n=20)
top2_df_sus = pd.DataFrame(top2_words_sus)
top2_df_sus.columns=["Bi-gram", "Frequency"]
print('Sustained complaints bi-gram list: \n', top2_df_sus)
print('\n')
top2_words_unsus = get_top_n2_words(unsus_corpus, n=20)
top2_df_unsus = pd.DataFrame(top2_words_unsus)
top2_df_unsus.columns=["Bi-gram", "Frequency"]
print('Unsustained complaints bi-gram list: \n',top2_df_unsus)

# Barplot of most freq words
import seaborn as sns
sns.set(rc={'figure.figsize':(13,8)})
fig = plt.figure(1)
g_sus = sns.barplot(x="Bi-gram", y="Frequency", data=top2_df_sus, palette="Blues_d")
g_sus.set_xticklabels(g_sus.get_xticklabels(), rotation=45)
fig = plt.figure(2)
g_unsus = sns.barplot(x="Bi-gram", y="Frequency", data=top2_df_unsus, palette="Reds_d")
g_unsus.set_xticklabels(g_unsus.get_xticklabels(), rotation=45)

"""# Extract keywords based on TF-IDf scores in the corpus"""

# Tokenize the text and build a vocabulary of known words
from sklearn.feature_extraction.text import CountVectorizer
import re
cv_sus = CountVectorizer(max_df=0.8, stop_words=stop_words, max_features=10000, ngram_range=(1,3))
X_sus = cv_sus.fit_transform(sus_corpus)
cv_unsus = CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))
X_unsus = cv_unsus.fit_transform(unsus_corpus)

# Get TF-IDF (term frequency/inverse document frequency) -- 
# TF-IDF lists word frequency scores that highlight words that 
# are more important to the context rather than those that 
# appear frequently across documents

from sklearn.feature_extraction.text import TfidfTransformer 
tfidf_transformer= TfidfTransformer(smooth_idf=True, use_idf=True)
tfidf_transformer.fit(X_sus)
tfidf_transformer.fit(X_unsus)

# Get feature names
sus_feature_names=cv_sus.get_feature_names()
unsus_feature_names=cv_unsus.get_feature_names()

# Fetch document for which keywords needs to be extracted
sus_doc=sus_corpus[sus_ds_count-1]
unsus_doc=unsus_corpus[unsus_ds_count-1]

# Generate tf-idf for the given document
sus_tf_idf_vector=tfidf_transformer.transform(cv_sus.transform([sus_doc]))
unsus_tf_idf_vector=tfidf_transformer.transform(cv_unsus.transform([unsus_doc]))

# Sort tf_idf in descending order
from scipy.sparse import coo_matrix

def sort_coo(coo_matrix):
    tuples = zip(coo_matrix.col, coo_matrix.data)
    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)
 
def extract_topn_from_vector(feature_names, sorted_items, topn=25):
    
    # Use only topn items from vector
    sorted_items = sorted_items[:topn]
    score_vals = []
    feature_vals = []
    
    # Word index and corresponding tf-idf score
    for idx, score in sorted_items:
        
        # Keep track of feature name and its corresponding score
        score_vals.append(round(score, 3))
        feature_vals.append(feature_names[idx])
 
    # Create tuples of feature,score
    # Results = zip(feature_vals,score_vals)
    results= {}
    for idx in range(len(feature_vals)):
        results[feature_vals[idx]]=score_vals[idx]
    return results

# Sort the tf-idf vectors by descending order of scores
sus_sorted_items=sort_coo(sus_tf_idf_vector.tocoo())
unsus_sorted_items=sort_coo(unsus_tf_idf_vector.tocoo())

# Extract only the top n; n here is 25
sus_keywords=extract_topn_from_vector(sus_feature_names,sus_sorted_items,25)
unsus_keywords=extract_topn_from_vector(unsus_feature_names,unsus_sorted_items,25)

# Print the results
print("\nAbstract:")
print(sus_doc)
print("\nKeywords:")
for k in sus_keywords:
    print(k, sus_keywords[k])
    
print("\nAbstract:")
print(unsus_doc)
print("\nKeywords:")
for k in unsus_keywords:
    print(k, unsus_keywords[k])


# import csv
# with open(file_prefix + 'td_idf.csv', 'w', newline="") as csv_file:  
#     writer = csv.writer(csv_file)
#     writer.writerow(["Keyword", "Importance"])
#     for key, value in keywords.items():
#        writer.writerow([key, value])

# Plot sustained TF-IDF
names_sus = list(sus_keywords.keys())
values_sus = list(sus_keywords.values())


plt.barh(range(len(sus_keywords)), values_sus, tick_label=names_sus)
plt.show()

# Plot unsustained TF-IDF
names_unsus = list(unsus_keywords.keys())
values_unsus = list(unsus_keywords.values())

plt.barh(range(len(unsus_keywords)), values_unsus, tick_label=names_unsus)
plt.show()

